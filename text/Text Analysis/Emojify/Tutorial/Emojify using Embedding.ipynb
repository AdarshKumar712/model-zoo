{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in this tutorial, we will be applying LSTM model to predict the emoji for various sentences. We will do this using pre-trained embeddings. This is quite similar to what one do in Text Sentiment Analysis where we will be predicting Sentiments in the form of Emojis.<br> <br>\n",
    "Let's first import all the required packages that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /home/blackforest/.julia/packages/CUDAdrv/mCr0O/src/CUDAdrv.jl:69\n",
      "WARNING: using StatsBase.crossentropy in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux:onehot,crossentropy,onecold\n",
    "using CSV,MLBase\n",
    "using Base.Iterators: repeated\n",
    "using DataFrames,StatsBase\n",
    "using PyCall,BSON\n",
    "using Embeddings\n",
    "using WordTokenizers\n",
    "using MLDataPattern\n",
    "using Random\n",
    "using MLDataUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have imported the required packages, it's time to load our data from the CSV file.\n",
    "Here our dataset file 'train.csv' consist of certain sentences and then the marking to the respective emoji class index which can be applied to that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 3 / 4 columns on data row: 132. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Text</th><th>Classifier</th></tr><tr><th></th><th>String</th><th>Int64</th></tr></thead><tbody><p>6 rows × 2 columns</p><tr><th>1</th><td>never talk to me again</td><td>3</td></tr><tr><th>2</th><td>I am proud of your achievements</td><td>2</td></tr><tr><th>3</th><td>It is the worst day in my life</td><td>3</td></tr><tr><th>4</th><td>Miss you so much</td><td>0</td></tr><tr><th>5</th><td>food is life</td><td>4</td></tr><tr><th>6</th><td>I love you mum</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& Text & Classifier\\\\\n",
       "\t\\hline\n",
       "\t& String & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & never talk to me again & 3 \\\\\n",
       "\t2 & I am proud of your achievements & 2 \\\\\n",
       "\t3 & It is the worst day in my life & 3 \\\\\n",
       "\t4 & Miss you so much & 0 \\\\\n",
       "\t5 & food is life & 4 \\\\\n",
       "\t6 & I love you mum & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "6×2 DataFrame\n",
       "│ Row │ Text                            │ Classifier │\n",
       "│     │ \u001b[90mString\u001b[39m                          │ \u001b[90mInt64\u001b[39m      │\n",
       "├─────┼─────────────────────────────────┼────────────┤\n",
       "│ 1   │ never talk to me again          │ 3          │\n",
       "│ 2   │ I am proud of your achievements │ 2          │\n",
       "│ 3   │ It is the worst day in my life  │ 3          │\n",
       "│ 4   │ Miss you so much                │ 0          │\n",
       "│ 5   │ food is life                    │ 4          │\n",
       "│ 6   │ I love you mum                  │ 0          │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd(\"./Downloads/emojify\")\n",
    "train = CSV.read(\"train_emoji.csv\",header =[\"Text\",\"Classifier\",\"Col3\",\"Col4\"])[:,[1,2]];\n",
    "first(train,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[:,1]\n",
    "Y = train[:,2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,Int64} with 5 entries:\n",
       "  0 => 22\n",
       "  4 => 17\n",
       "  2 => 38\n",
       "  3 => 36\n",
       "  1 => 19"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countmap(train[!,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it's quite clear from above, that out dataset classes are skewed with two classes dominating over other. So here, we will be applying oversampling. What Oversample does is, it create copies of the less represented classes in the dataset and make all the classes equally represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal,Y_bal = oversample((X,Y),shuffle = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does these indexes represent???<br>\n",
    "Let's define a dictionary of specify what these indexes actually signify. <br>\n",
    "\n",
    "<i>Note: This is taken from the dataset source</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,String} with 5 entries:\n",
       "  0 => \"💙\"\n",
       "  4 => \"🍴\"\n",
       "  2 => \"😄\"\n",
       "  3 => \"😞\"\n",
       "  1 => \"🎾\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_dictionary = Dict{Int64,String}(0=>\"💙\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    1=> \"🎾\",\n",
    "                    2=> \"😄\",\n",
    "                    3=> \"😞\",\n",
    "                    4=> \"🍴\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to change our target dataset as one hot label. Let's do that real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "Y_ = zeros(N,length(Y_bal))\n",
    "for i in 1:length(Y_bal)\n",
    "    Y_[Y_bal[i]+1,i] = 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified earlier, we will use here pre-trained text embeddings based on GloVe dataset. If, you are running this for the first time, this may take a bit longer as GloVe need to get downloaded first. But once that's done, it won't be needed to download again. There will be only some loading time........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenize (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const embtable = load_embeddings(GloVe) # or load_embeddings(FastText_Text) or ...\n",
    "\n",
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(embtable.vocab))\n",
    "\n",
    "function get_embedding(word)\n",
    "    ind = get_word_index[word]\n",
    "    emb = embtable.embeddings[:,ind]\n",
    "    return emb\n",
    "end\n",
    "\n",
    "set_tokenizer(poormans_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need to tokenize that sentences, which we will afterwards convert using text embeddings loaded above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = [tokenize(lowercase(i)) for i in X_bal];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenise (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tokenise(s)\n",
    "    token_arr = []\n",
    "    for c in s\n",
    "        if (c in keys(get_word_index))==1\n",
    "        push!(token_arr,get_embedding(c))\n",
    "    else\n",
    "        #print(c)\n",
    "        push!(token_arr,get_embedding(\"unk\"))\n",
    "    \n",
    "        end\n",
    "    end\n",
    "    return token_arr\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [tokenise(a) for a in X_];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for i in 1:length(Xs)\n",
    "    if max_length<length(Xs[i])\n",
    "        max_length = length(Xs[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before moving further, we first need to convert the Data, an array of data elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting array{array{array,1],1},1}-Xs to array{embedded matrix}  \n",
    "X1 = []\n",
    "for i in 1:length(Xs)\n",
    "    m = fill(0.0,(length(Xs[i][1]),max_length))\n",
    "    for j =1:length(Xs[i])\n",
    "        for k in 1:length(Xs[i][j])\n",
    "            m[k,j] = Xs[i][j][k]\n",
    "        end\n",
    "    end\n",
    "    push!(X1,m)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = []\n",
    "for i in 1:size(Y_)[2]\n",
    "    push!(Y1,Y_[:,i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "Normalisation is a very important aspect in order to improve the performance of any model. It allows model to converge more easily and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:length(X)\n",
    "    X1[i] = Flux.normalise(X1[i],dims = 2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,Y1 = shuffleobs((X1,Y1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X1[1:176]\n",
    "Y_train = Y1[1:176]\n",
    "X_val = X1[161:end]\n",
    "Y_val = Y1[161:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the data\n",
    "As here, we will be using the Stochastic gradient process for training, so we will be updating our model after each element.\n",
    "Hence, we will have just one element in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []  #batching of each element for Stochastic Gradient Descent as [(X1[1],Y1[1]),(X1[2],Y1[2].......)]\n",
    "for i in 1:length(X_train)\n",
    "    push!(batch,(X_train[i],[Y_train[i]]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Once, we are done, It's time making our model. Here we will use Chain method of model declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Recur(LSTMCell(50, 128)), LayerNorm(128), #35, Dropout(0.5), Recur(LSTMCell(128, 128)), LayerNorm(128), Dropout(0.5), #36, Dense(128, 5), softmax)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model\n",
    "Scanner = Chain(LSTM(length(Xs[1][1]),128),LayerNorm(128),x->relu.(x),\n",
    "        Dropout(0.5),        \n",
    "        LSTM(128,128),\n",
    "        LayerNorm(128),\n",
    "        Dropout(0.5),\n",
    "        x->relu.(x),\n",
    "        Dense(128,N)\n",
    "        ,softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "In the loss function, we will be evaluating the crossentropy loss. Another important point to note here is that we will be reseting the hidden state, after evaluation of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(x,y)\n",
    "    y_hat = Scanner.(x)\n",
    "    l= crossentropy(y_hat[1][:,end],y[1])\n",
    "    Flux.reset!(Scanner)\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.520803928375244"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(batch[1]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for performance analysis\n",
    "This is the loss function that we will be using after each iteration to evaluate the performance of the model\n",
    "Here, an important aspect to note is that after each model production, we need to reset the hidden state, as otherwise they may get transferred as initial state for next prediction, which is definitely not wha we want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "val_loss_ (generic function with 1 method)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function val_loss_(x,y)\n",
    "    y_ = []\n",
    "    for i in 1:length(x)\n",
    "        push!(y_,Scanner.(x[i]))\n",
    "        Flux.reset!(Scanner)\n",
    "    end\n",
    "    y_hat = y_\n",
    "    l=0.0\n",
    "    for i in 1:length(y_hat)\n",
    "        l+= crossentropy(y_hat[i][length(x[i])][:,end],y[i])\n",
    "    end\n",
    "    return l/length(y_hat)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function accuracy(x,y)\n",
    "    y_hat = []\n",
    "    for i in 1:length(x)\n",
    "        push!(y_hat,Scanner.(x[i]))\n",
    "        Flux.reset!(Scanner)\n",
    "    end\n",
    "    sum=0.0\n",
    "    for i in 1:length(y)\n",
    "        if (argmax(y_hat[i][length(x[i])][:,end])==argmax(y[i]))\n",
    "        sum+=1.0\n",
    "        end\n",
    "    end\n",
    "    return sum/length(y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Once we are done with all the preprocessing, let's begin the model training.........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM(0.0001,(0.9,0.999))\n",
    "epochs = 100\n",
    "\n",
    "best_acc = 0\n",
    "@info(\"Beginning training loop...\")\n",
    "\n",
    "for i in 1:epochs\n",
    "    global best_acc\n",
    "    index = Random.randperm(length(batch))\n",
    "    batch = batch[index]\n",
    "    Flux.train!(loss, params(Scanner),batch, opt)\n",
    "    loss_val = val_loss_(X_val,Y_val)\n",
    "    accuracy_ = accuracy(X_val,Y_val)\n",
    "    if i%10==0\n",
    "        print(\"Epoch[$i]- Loss: $loss_val Accuracy: $accuracy_\\n\")\n",
    "    end\n",
    "    # If this is the best accuracy we've seen so far, save the model out\n",
    "    if accuracy_ >best_acc\n",
    "        @info(\"Epoch[$i]-> New best accuracy: $accuracy_ ! Saving model out to emojifier_norm.bson\")\n",
    "        BSON.@save joinpath(dirname(@__FILE__), \"emojifier_norm.bson\") Scanner max_length\n",
    "        best_acc = accuracy_\n",
    "    end\n",
    "            \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is done, let's have a look at the performance on training data.<br><br>\n",
    "Let's start with Model accuracy........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7526315789473684"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model evaluation on Training Data\n",
    "BSON.@load \"./emojifier_norm.bson\" Scanner\n",
    "accuracy(X1,Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = []\n",
    "for i in 1:length(X1)\n",
    "    push!(y_label,Scanner(X1[i]))\n",
    "    Flux.reset!(Scanner)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the confusion matrix of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       " 4  4  7   4  19\n",
       " 3  5  7   5  18\n",
       " 7  4  6   5  16\n",
       " 9  3  4  10  12\n",
       " 7  5  6   3  17"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "for i in 1:length(y_label)\n",
    "    push!(y_pred,Int64(argmax(y_label[i][:,10])-1))\n",
    "    end\n",
    "y_pred = Int64.(y_pred);\n",
    "\n",
    "\n",
    "confusmat(5,Y_bal.+1,y_pred.+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the model is highly biased towards the last class. This model need to be optimized further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
